[![Generated by Claude 3.7 Sonnet](https://img.shields.io/badge/Generated%20by-Claude%203.7%20Sonnet-blue)](https://www.anthropic.com/)

# Energy Transformer - PyTorch

A PyTorch implementation of Energy Transformer, a novel architecture that combines the properties of Transformers, Energy-Based Models, and Associative Memory.

## Overview

Energy Transformer (ET) is a continuous dynamical system with a tractable energy function. The forward pass is performed via gradient descent on this energy, making it fundamentally different from standard transformers while achieving similar or better performance with improved parameter efficiency.

## Key Features

- **Specification API**: Declarative model definition using composable specifications
- **Energy-based formulation**: Forward pass via energy minimization
- **Parameter efficient**: Achieves strong performance with fewer parameters
- **Interpretable**: Energy landscape provides insights into model behavior
- **Flexible**: Can be used for various tasks including image reconstruction and classification

## Installation

```bash
git clone https://github.com/b-vitamins/energy-transformer
cd energy-transformer
pip install -e .
```

## Quick Start

### Using Pre-defined Models

```python
import torch
from energy_transformer import realise, vit_base

# Create a Vision Transformer with Energy blocks
model = realise(vit_base(img_size=224, patch_size=16))

# Forward pass
images = torch.randn(4, 3, 224, 224)
output = model(images)  # (4, 768) - CLS token representation
```

### Custom Architecture with Specifications

```python
from energy_transformer import seq, repeat, PatchSpec, PosEncSpec, CLSTokenSpec, ETBlockSpec, NormSpec, realise

# Define model architecture declaratively
spec = seq(
    PatchSpec(img_size=224, patch_size=16, in_chans=3, embed_dim=768),
    PosEncSpec(kind="sincos"),
    CLSTokenSpec(),
    repeat(ETBlockSpec(steps=4, alpha=0.125), 12),
    NormSpec()
)

# Create the actual model
model = realise(spec)
```

### Image Processing with Masking

```python
from energy_transformer import realise, mae_base

# Create model with mask token support
model = realise(mae_base(img_size=224, patch_size=16))

# Process images with masking
images = torch.randn(4, 3, 224, 224)
mask = torch.bernoulli(torch.full((4, 196), 0.75))  # 75% masking
results = model(images, patch_mask=mask, return_sequence=True)
```

### Direct Model Usage

```python
from energy_transformer import EnergyTransformer
from energy_transformer.layers import LayerNorm, MultiHeadEnergyAttention, HopfieldNetwork

# Create Energy Transformer block
et_block = EnergyTransformer(
    layer_norm=LayerNorm(768),
    attention=MultiHeadEnergyAttention(in_dim=768, num_heads=12, head_dim=64),
    hopfield=HopfieldNetwork(in_dim=768, hidden_dim=2048),
    steps=4,
    alpha=0.125
)

# Forward pass
x = torch.randn(2, 10, 768)  # (batch_size, seq_len, d_model)
output = et_block(x)
```

## Model Architecture

The Energy Transformer consists of three main components:

1. **Energy Layer Normalization**: Modified layer normalization with Lagrangian formulation
2. **Energy Attention**: Multi-head attention mechanism based on energy minimization
3. **Hopfield Network**: Associative memory that replaces traditional MLP

The total energy is the sum of individual component energies, and the forward pass minimizes this energy via gradient descent.

## Pre-defined Configurations

```python
from energy_transformer import vit_tiny, vit_small, vit_base, vit_large, mae_base

tiny_model = realise(vit_tiny())    # 12 layers, 192 dim
small_model = realise(vit_small())  # 12 layers, 384 dim  
base_model = realise(vit_base())    # 12 layers, 768 dim
large_model = realise(vit_large())  # 24 layers, 1024 dim
mae_model = realise(mae_base())     # For masked autoencoding
```

## Testing

Run tests with pytest:

```bash
pytest tests/
```

## Citation

If you use this code in your research, please cite the original Energy Transformer paper:

```bibtex
@inproceedings{10.5555/3666122.3667319,
author = {Hoover, Benjamin and Liang, Yuchen and Pham, Bao and Panda, Rameswar and Strobelt, Hendrik and Chau, Duen Horng and Zaki, Mohammed J. and Krotov, Dmitry},
title = {Energy transformer},
year = {2023},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
articleno = {1197},
numpages = {28},
location = {New Orleans, LA, USA},
series = {NIPS '23}
}
```

## License

This project is licensed under the Apache License 2.0 - see the LICENSE file for details.

## Acknowledgments

This unofficial PyTorch port of the [official JAX implementation](https://github.com/bhoov/energy-transformer-jax) was generated entirely by Claude 3.7 Sonnet ([Anthropic](https://www.anthropic.com)).