[![Generated by Claude 3.7 Sonnet](https://img.shields.io/badge/Generated%20by-Claude%203.7%20Sonnet-blue)](https://www.anthropic.com/)

# Energy Transformer - PyTorch

A PyTorch implementation of Energy Transformer, a novel architecture that combines the properties of Transformers, Energy-Based Models, and Associative Memory.

## Overview

Energy Transformer (ET) is a continuous dynamical system with a tractable energy function. The forward pass is performed via gradient descent on this energy, making it fundamentally different from standard transformers while achieving similar or better performance with improved parameter efficiency.

## Key Features

- **Specification API**: Declarative model definition using composable specifications
- **Energy-based formulation**: Forward pass via energy minimization
- **Parameter efficient**: Achieves strong performance with fewer parameters
- **Interpretable**: Energy landscape provides insights into model behavior
- **Flexible**: Can be used for various tasks including image reconstruction and classification

## Installation

```bash
git clone https://github.com/b-vitamins/energy-transformer
cd energy-transformer
pip install -e .
```

## Quick Start

### Basic Usage

```python
import torch
from energy_transformer import realise, seq, repeat
from energy_transformer.spec import (
    PatchEmbedSpec,
    PosEmbedSpec,
    CLSTokenSpec,
    ETSpec,
    LayerNormSpec,
)

spec = seq(
    PatchEmbedSpec(img_size=224, patch_size=16, in_chans=3, embed_dim=768),
    PosEmbedSpec(),
    CLSTokenSpec(),
    repeat(ETSpec(steps=4, alpha=0.125), 12),
    LayerNormSpec(),
)

model = realise(spec)

images = torch.randn(4, 3, 224, 224)
output = model(images)
```

### Custom Architecture with Specifications

```python
from energy_transformer import realise, seq, repeat
from energy_transformer.spec import (
    PatchEmbedSpec,
    PosEmbedSpec,
    CLSTokenSpec,
    ETSpec,
    LayerNormSpec,
)

spec = seq(
    PatchEmbedSpec(img_size=224, patch_size=16, in_chans=3, embed_dim=768),
    PosEmbedSpec(kind="sincos"),
    CLSTokenSpec(),
    repeat(ETSpec(steps=4, alpha=0.125), 12),
    LayerNormSpec(),
)

model = realise(spec)
```

### Direct Model Usage

```python
from energy_transformer import EnergyTransformer
from energy_transformer.layers import LayerNorm, MultiHeadEnergyAttention, HopfieldNetwork

# Create Energy Transformer block
et_block = EnergyTransformer(
    layer_norm=LayerNorm(768),
    attention=MultiHeadEnergyAttention(in_dim=768, num_heads=12, head_dim=64),
    hopfield=HopfieldNetwork(in_dim=768, hidden_dim=2048),
    steps=4,
    alpha=0.125
)

# Forward pass
x = torch.randn(2, 10, 768)  # (batch_size, seq_len, d_model)
output = et_block(x)
```

## Model Architecture

The Energy Transformer consists of three main components:

1. **Energy Layer Normalization**: Modified layer normalization with Lagrangian formulation
2. **Energy Attention**: Multi-head attention mechanism based on energy minimization
3. **Hopfield Network**: Associative memory that replaces traditional MLP

The total energy is the sum of individual component energies, and the forward pass minimizes this energy via gradient descent.


## Testing

Run tests with pytest:

```bash
pytest tests/
```

## Citation

If you use this code in your research, please cite the original Energy Transformer paper:

```bibtex
@inproceedings{10.5555/3666122.3667319,
author = {Hoover, Benjamin and Liang, Yuchen and Pham, Bao and Panda, Rameswar and Strobelt, Hendrik and Chau, Duen Horng and Zaki, Mohammed J. and Krotov, Dmitry},
title = {Energy transformer},
year = {2023},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
articleno = {1197},
numpages = {28},
location = {New Orleans, LA, USA},
series = {NIPS '23}
}
```

## License

This project is licensed under the Apache License 2.0 - see the LICENSE file for details.

## Acknowledgments

This unofficial PyTorch port of the [official JAX implementation](https://github.com/bhoov/energy-transformer-jax) was generated entirely by Claude 3.7 Sonnet ([Anthropic](https://www.anthropic.com)).
