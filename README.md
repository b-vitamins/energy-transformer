[![Generated by Claude 3.7 Sonnet](https://img.shields.io/badge/Generated%20by-Claude%203.7%20Sonnet-blue)](https://www.anthropic.com/)

# Energy Transformer - PyTorch

A PyTorch implementation of Energy Transformer, a novel architecture that combines the properties of Transformers, Energy-Based Models, and Associative Memory.

## Overview

Energy Transformer (ET) is a continuous dynamical system with a tractable energy function. The forward pass is performed via gradient descent on this energy, making it fundamentally different from standard transformers while achieving similar or better performance with improved parameter efficiency.

## Key Features

- **Energy-based formulation**: Forward pass via energy minimization
- **Parameter efficient**: Achieves strong performance with fewer parameters
- **Interpretable**: Energy landscape provides insights into model behavior
- **Flexible**: Can be used for various tasks including image reconstruction and classification

## Installation

```bash
git clone https://github.com/b-vitamins/energy-transformer
cd energy-transformer
pip install -e .
```

## Quick Start

### Basic Usage

```python
import torch
from energy_transformer import ETConfig, EnergyTransformer

# Create configuration
config = ETConfig(
    d_model=768,
    d_head=64,
    n_heads=12,
    scale_memories=4.0,
    alpha=0.1,
    n_steps=12
)

# Initialize model
model = EnergyTransformer(config)

# Forward pass
x = torch.randn(2, 10, 768)  # (batch_size, seq_len, d_model)
output = model(x)

# Track energy during forward pass
output, trajectory = model(x, return_trajectory=True)
final_state, energies = model.compute_energy_trajectory(x)
```

### Image Processing

```python
from energy_transformer import ImageETConfig, ImageEnergyTransformer

# Configure for images
config = ImageETConfig(
    image_shape=(3, 224, 224),
    patch_size=16,
    n_mask=100
)

# Create model
model = ImageEnergyTransformer(config)

# Process images
images = torch.randn(4, 3, 224, 224)
mask = model.create_random_mask(4, images.device)
results = model(images, mask=mask)
reconstructed = results['reconstruction']
```

## Model Architecture

The Energy Transformer consists of three main components:

1. **EnergyLayerNorm**: Modified layer normalization with Lagrangian formulation
2. **EnergyAttention**: Attention mechanism based on energy minimization
3. **HopfieldNetwork**: Replaces MLP with associative memory

The total energy is the sum of individual component energies, and the forward pass minimizes this energy via gradient descent.

## Examples

See the `examples/` (coming soon) directory for complete examples:

- `basic_tutorial.py`: Demonstrates energy minimization (coming soon)
- `image_reconstruction.py`: Image masking and reconstruction (coming soon)
- `train_imagenet.py`: Training on ImageNet (coming soon)

## Testing

Run tests with pytest:

```bash
pytest tests/
```

## Citation

If you use this code in your research, please cite the original Energy Transformer paper:

```bibtex
@inproceedings{10.5555/3666122.3667319,
author = {Hoover, Benjamin and Liang, Yuchen and Pham, Bao and Panda, Rameswar and Strobelt, Hendrik and Chau, Duen Horng and Zaki, Mohammed J. and Krotov, Dmitry},
title = {Energy transformer},
year = {2023},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
articleno = {1197},
numpages = {28},
location = {New Orleans, LA, USA},
series = {NIPS '23}
}
```

## License

This project is licensed under the Apache License 2.0 - see the LICENSE file for details.

## Acknowledgments
This unofficial PyTorch port of the [official JAX implementation](https://github.com/bhoov/energy-transformer-jax) was generated entirely by Claude 3.7 Sonnet ([Anthropic](https://www.anthropic.com)).
